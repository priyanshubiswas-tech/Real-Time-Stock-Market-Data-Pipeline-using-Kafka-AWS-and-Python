# ğŸ“ˆ Real-Time Stock Market Data Pipeline using Kafka, AWS, and Python

This project demonstrates a real-time streaming pipeline that simulates stock market data, processes it with Kafka, stores it in AWS S3, and queries it using AWS Athena.

---

## ğŸ¯ What You Will Learn

ğŸ‘‰ Build a real-time simulation app using Python
ğŸ‘‰ Understand Kafka fundamentals (Broker, Producer, Consumer, Zookeeper)
ğŸ‘‰ Install and configure Kafka on an EC2 instance
ğŸ‘‰ Write Kafka Producer and Consumer using Python
ğŸ‘‰ Stream and store data to AWS S3
ğŸ‘‰ Query real-time data using AWS Athena

---

## ğŸ— Architecture Diagram

```
+-------------+       +-------------+       +-------------+       +-------------+
| Stock Data  | --->  | Kafka       | --->  | Consumer    | --->  | S3 Bucket    |
| Simulator   |       | (Broker)    |       | (Python)    |       | (CSV/Parquet)|
+-------------+       +-------------+       +-------------+       +-------------+
                                                                 |
                                                                 v
                                                          +-------------+
                                                          | AWS Athena  |
                                                          +-------------+
```

---

## ğŸ“ Folder Structure

```
real-time-stock-data-pipeline/
ğŸŒ€
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ kafka_installation.md         # Kafka + Zookeeper setup guide
â”‚   â”œâ”€â”€ start_kafka.sh                # Script to start Kafka and Zookeeper
â”‚   â””â”€â”€ create_topic.sh               # Script to create Kafka topic
â”‚
â”œâ”€â”€ producer/
â”‚   â””â”€â”€ producer.py                   # Simulates stock data and sends to Kafka
â”‚
â”œâ”€â”€ consumer/
â”‚   â””â”€â”€ consumer_to_s3.py             # Consumes Kafka data and uploads to S3
â”‚
â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ s3_bucket_setup.md            # How to create an S3 bucket
â”‚   â”œâ”€â”€ athena_query.sql              # SQL to create Athena table
â”‚   â””â”€â”€ iam_policy.json               # IAM policy for S3 and Athena access
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ stock_data.csv                # Sample CSV for local testing
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ requirements.txt              # Python dependencies
â”‚   â””â”€â”€ setup_env.sh                  # Optional env setup script
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ architecture_diagram.png      # Visual overview of the architecture
â”‚
â”œâ”€â”€ .gitignore                        # Ignore __pycache__, .env, etc.
â”œâ”€â”€ README.md                         # This documentation
â””â”€â”€ LICENSE                           # Project license (MIT)
```

---

## âœ¨ Deliverables

| Category       | Description                                                               |
| -------------- | ------------------------------------------------------------------------- |
| ğŸ’» Code        | Python scripts for producer (stock simulation) and consumer (S3 uploader) |
| âš™ Kafka        | Scripts to install Kafka, start services, and create topics               |
| â˜ AWS Config   | Athena SQL query, S3 setup guide, IAM policy                              |
| ğŸ§ª Sample Data | Sample `CSV` for testing without Kafka/S3                                 |
| ğŸ“„ Docs        | README, architecture diagram, and optional setup guides                   |
| ğŸ“¦ Scripts     | Dependency file (`requirements.txt`) and environment setup script         |

---

## ğŸš€ Getting Started

### 1. Kafka Installation on EC2

```bash
sudo apt update && sudo apt upgrade -y
sudo apt install default-jdk -y

wget https://downloads.apache.org/kafka/3.0.0/kafka_2.13-3.0.0.tgz
tar -xvzf kafka_2.13-3.0.0.tgz
cd kafka_2.13-3.0.0

# Start Zookeeper
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka Broker
bin/kafka-server-start.sh config/server.properties
```

### 2. Create Kafka Topic

```bash
bin/kafka-topics.sh --create \
--topic stock-data \
--bootstrap-server localhost:9092 \
--partitions 1 --replication-factor 1
```

---

## ğŸ§¬ Python Code

### â” `producer/producer.py`

```python
from kafka import KafkaProducer
import json, time, random

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

tickers = ['AAPL', 'GOOG', 'TSLA', 'MSFT']

while True:
    data = {
        'ticker': random.choice(tickers),
        'price': round(random.uniform(100, 1500), 2),
        'timestamp': time.time()
    }
    producer.send('stock-data', value=data)
    print("Produced:", data)
    time.sleep(1)
```

### â” `consumer/consumer_to_s3.py`

```python
from kafka import KafkaConsumer
import json, boto3, csv, os
from datetime import datetime

consumer = KafkaConsumer(
    'stock-data',
    bootstrap_servers='localhost:9092',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

s3 = boto3.client('s3')
bucket_name = 'your-s3-bucket-name'
local_file = 'stock_data.csv'

if not os.path.exists(local_file):
    with open(local_file, 'w', newline='') as f:
        csv.writer(f).writerow(['ticker', 'price', 'timestamp'])

for message in consumer:
    data = message.value
    with open(local_file, 'a', newline='') as f:
        csv.writer(f).writerow([data['ticker'], data['price'], datetime.fromtimestamp(data['timestamp'])])
    s3.upload_file(local_file, bucket_name, f'stock_data/{datetime.now().isoformat()}.csv')
    print("Uploaded:", data)
```

---

## â˜ï¸ AWS Integration

### â” S3 Setup

* Create a new S3 bucket
* Name it `your-s3-bucket-name`
* Allow write permissions via IAM

### â” IAM Policy (`aws/iam_policy.json`)

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:PutObject", "s3:GetObject", "s3:ListBucket"],
      "Resource": ["arn:aws:s3:::your-s3-bucket-name/*"]
    }
  ]
}
```

### â” Athena Table Creation (`aws/athena_query.sql`)

```sql
CREATE EXTERNAL TABLE stock_data (
  ticker string,
  price double,
  timestamp string
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'separatorChar' = ',',
  'quoteChar' = '"'
)
LOCATION 's3://your-s3-bucket-name/stock_data/'
TBLPROPERTIES ('has_encrypted_data'='false');
```

---

## ğŸ“¦ Requirements

### â” `scripts/requirements.txt`

```
kafka-python
boto3
```

### â” (Optional) Virtual Environment Setup

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r scripts/requirements.txt
```

---

## ğŸ“œ References

* [Kafka Documentation](https://kafka.apache.org/documentation/)
* [Kafka-Python Client](https://kafka-python.readthedocs.io/en/master/)
* [AWS S3 Docs](https://docs.aws.amazon.com/s3/)
* [AWS Athena Docs](https://docs.aws.amazon.com/athena/)

---

## ğŸ§  Future Improvements

* Stream data using Apache Avro or Parquet for better efficiency
* Integrate AWS Glue for schema discovery
* Visualize data using Amazon QuickSight or Streamlit
* Add monitoring/logging with Prometheus + Grafana
* Dockerize the whole system for reproducibility

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).
